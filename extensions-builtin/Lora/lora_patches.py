_B='_load_from_state_dict'
_A='forward'
import torch,networks
from modules import patches
class LoraPatches:
	def __init__(A):A.Linear_forward=patches.patch(__name__,torch.nn.Linear,_A,networks.network_Linear_forward);A.Linear_load_state_dict=patches.patch(__name__,torch.nn.Linear,_B,networks.network_Linear_load_state_dict);A.Conv2d_forward=patches.patch(__name__,torch.nn.Conv2d,_A,networks.network_Conv2d_forward);A.Conv2d_load_state_dict=patches.patch(__name__,torch.nn.Conv2d,_B,networks.network_Conv2d_load_state_dict);A.GroupNorm_forward=patches.patch(__name__,torch.nn.GroupNorm,_A,networks.network_GroupNorm_forward);A.GroupNorm_load_state_dict=patches.patch(__name__,torch.nn.GroupNorm,_B,networks.network_GroupNorm_load_state_dict);A.LayerNorm_forward=patches.patch(__name__,torch.nn.LayerNorm,_A,networks.network_LayerNorm_forward);A.LayerNorm_load_state_dict=patches.patch(__name__,torch.nn.LayerNorm,_B,networks.network_LayerNorm_load_state_dict);A.MultiheadAttention_forward=patches.patch(__name__,torch.nn.MultiheadAttention,_A,networks.network_MultiheadAttention_forward);A.MultiheadAttention_load_state_dict=patches.patch(__name__,torch.nn.MultiheadAttention,_B,networks.network_MultiheadAttention_load_state_dict)
	def undo(A):A.Linear_forward=patches.undo(__name__,torch.nn.Linear,_A);A.Linear_load_state_dict=patches.undo(__name__,torch.nn.Linear,_B);A.Conv2d_forward=patches.undo(__name__,torch.nn.Conv2d,_A);A.Conv2d_load_state_dict=patches.undo(__name__,torch.nn.Conv2d,_B);A.GroupNorm_forward=patches.undo(__name__,torch.nn.GroupNorm,_A);A.GroupNorm_load_state_dict=patches.undo(__name__,torch.nn.GroupNorm,_B);A.LayerNorm_forward=patches.undo(__name__,torch.nn.LayerNorm,_A);A.LayerNorm_load_state_dict=patches.undo(__name__,torch.nn.LayerNorm,_B);A.MultiheadAttention_forward=patches.undo(__name__,torch.nn.MultiheadAttention,_A);A.MultiheadAttention_load_state_dict=patches.undo(__name__,torch.nn.MultiheadAttention,_B)